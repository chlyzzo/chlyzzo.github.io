
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>tensorflow学习-word2vec | 山上掏金</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    <meta name="baidu-site-verification" content="Poq2hMEF9U" />
    
    <meta name="author" content="chloy">
    

    
    <meta name="description" content="tensorflow学习笔记系列原始内容，可从CS 20SI: Tensorflow for Deep Learning Research。另还有几本关于tensorflow的书籍，比如tensorflow实战，tensorflow解析等。感谢把知识分享出来的各位大牛。 word2vec词转成向量的好处是可以把抽象的词数字化，从而在模型（数学公式）中可应用；word2vec是一种词嵌入编码，与之相">
<meta name="keywords" content="笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="tensorflow学习-word2vec">
<meta property="og:url" content="https://chlyzzo.github.io/2017/09/28/tensorflow学习-word2vec/index.html">
<meta property="og:site_name" content="山上掏金">
<meta property="og:description" content="tensorflow学习笔记系列原始内容，可从CS 20SI: Tensorflow for Deep Learning Research。另还有几本关于tensorflow的书籍，比如tensorflow实战，tensorflow解析等。感谢把知识分享出来的各位大牛。 word2vec词转成向量的好处是可以把抽象的词数字化，从而在模型（数学公式）中可应用；word2vec是一种词嵌入编码，与之相">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2017-10-27T07:21:12.762Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="tensorflow学习-word2vec">
<meta name="twitter:description" content="tensorflow学习笔记系列原始内容，可从CS 20SI: Tensorflow for Deep Learning Research。另还有几本关于tensorflow的书籍，比如tensorflow实战，tensorflow解析等。感谢把知识分享出来的各位大牛。 word2vec词转成向量的好处是可以把抽象的词数字化，从而在模型（数学公式）中可应用；word2vec是一种词嵌入编码，与之相">

    
    <link rel="alternative" href="/atom.xml" title="山上掏金" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/logo.png">
    <link rel="apple-touch-icon-precomposed" href="/img/logo.png">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <span id="busuanzi_container_site_pv">
    站浏览量<span id="busuanzi_value_site_pv"></span>次
  </span>
  <span id="busuanzi_container_site_uv">
    站访问人数<span id="busuanzi_value_site_uv"></span>
  </span>
<div>
		
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="山上掏金">山上掏金</a></h1>
				<h2 class="blog-motto">每天早上起床就是为了比昨天更快乐，掏金者的一天是新的开始.</h2>
			</div>                    

			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">主页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/about">作者</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:chlyzzo.github.io">
					</form>
					
					</li>
				</ul>
			</nav>
</div>

    </header>
    <div id="container">
      <div id="main" class="post moveMain" itemscope itemprop="blogPost">
  
	<article itemprop="articleBody">
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/09/28/tensorflow学习-word2vec/" title="tensorflow学习-word2vec" itemprop="url">tensorflow学习-word2vec</a>
  </h1>
  <div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/深度学习/">深度学习</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/笔记/">笔记</a>
  </div>

</div>


  <p class="article-author">By
       
		<a href="/about" title="chloy" target="_blank" itemprop="author">chloy</a>
		
  <p class="article-time">
    <time datetime="2017-09-28T07:05:07.000Z" itemprop="datePublished"> 2017-09-28 阅读量 <span id="busuanzi_value_page_pv"></span></time>
  </p>
</header>

	<div class="article-content">
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">目录</strong>
		
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#word2vec"><span class="toc-number">1.</span> <span class="toc-text">word2vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#skip-model训练过程"><span class="toc-number">2.</span> <span class="toc-text">skip-model训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nce-loss"><span class="toc-number">2.1.</span> <span class="toc-text">nce_loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练步骤"><span class="toc-number">2.2.</span> <span class="toc-text">训练步骤</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#代码"><span class="toc-number">3.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#应用"><span class="toc-number">4.</span> <span class="toc-text">应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型复用"><span class="toc-number">5.</span> <span class="toc-text">模型复用</span></a></li></ol>
		
		</div>
		
		<p>tensorflow学习笔记系列原始内容，可从<a href="http://web.stanford.edu/class/cs20si/syllabus.html" target="_blank" rel="noopener">CS 20SI: Tensorflow for Deep Learning Research</a>。另还有几本关于tensorflow的书籍，比如tensorflow实战，tensorflow解析等。感谢把知识分享出来的各位大牛。</p>
<h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>词转成向量的好处是可以把抽象的词数字化，从而在模型（数学公式）中可应用；word2vec是一种词嵌入编码，与之相当的有one-hot编码，即0/1标示；word2vec的理解是把词映射到一个多维空间内，相同或相似的词会在空间内离得很近。这种空间映射在不同语言的翻译中体现在，意思相同的词在各自语言的空间所处的位置是一样的。</p>
<p>总之，词嵌入的word2vec，是把词映射成一维向量，后续的应用可以基于向量来操作。</p>
<p>训练word2vec的方法有很多，两种常见的大类模型是skip-model和cbow-model；在skip-model中，是基于中心词得到其前后skip个词，组成词对作为训练样本；而cbow是在中心词的前后c个词与中心组成一个对作为正样本，而非中心词的都是负样本，NEG(w)。这里对cbow不细讲，主要讲skip-model的训练。</p>
<h2 id="skip-model训练过程"><a href="#skip-model训练过程" class="headerlink" title="skip-model训练过程"></a>skip-model训练过程</h2><p>首先，必须了解使用模型训练，要先给定样本、参数、损失函数、优化器。skip-model中样本是中心词前后skip个词构成的词对，参数是神经网络的权重，损失函数用nce，优化器是批梯度下降法。除了样本需要自定义生成外，其他的在tensorflow中均有封装函数可使用，这里会着重讲下nce损失函数，只是个人阅读后的见解。</p>
<h3 id="nce-loss"><a href="#nce-loss" class="headerlink" title="nce_loss"></a>nce_loss</h3><p>噪声对比估计损失，可想看<a href="http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf" target="_blank" rel="noopener">论文</a>,详见tensorflow的<a href="http://www.tensorfly.cn/tfdoc/api_docs/python/nn.html#nce_loss" target="_blank" rel="noopener">官方文档</a>介绍,以及<a href="https://github.com/tensorflow/tensorflow/blob/e55574f28257bdacd744dcdba86c839e661b1b2a/tensorflow/python/ops/nn_impl.py" target="_blank" rel="noopener">tensorflow的源码</a>,</p>
<p>nce是为了加快多分类的速度，多分类下需要对每个可能类计算概率（100万个每个样本就得计算100万个概率），而在nce中，可以随机选几个类，计算概率，然后使用逻辑函数进行计算其他类别的概率(具体操作未知)。</p>
<p>值得注意的几点：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1,num_true大于1，即预测目标的概率和为1；</span><br><span class="line">2，参数详解：</span><br><span class="line">weights,[num_classes,dim]的张量，dim是样本的特征数量，</span><br><span class="line">biases，[num_classes]的张量，</span><br><span class="line">inputs,[batch_size,dim]的张量，前向激活网络的输入，喂入模型的训练数据，中心词</span><br><span class="line">labels，[batch_size,num_true],int64,目标类，目标词，</span><br><span class="line">num_sampled,每个batch中随机选的类数量，在word2vec中即随机选的负样本数（不是中心词本身），</span><br><span class="line">num_classes,所有可能的类别数量，</span><br><span class="line">num_true,每个训练样本的目标类别数（真实的）</span><br><span class="line">sampled_values，采样的方法，</span><br><span class="line">remove_accidental_hits，样本如果与目标类一致是否删除，设计到计算 loss的方法。</span><br><span class="line"></span><br><span class="line">其中inputs与labels是配对关系，</span><br><span class="line"></span><br><span class="line">3，返回[batch_size,d]为的nce损失值，是一个向量。</span><br></pre></td></tr></table></figure></p>
<p>sigmoid_cross_entropy_with_logits，多目标，可属于多个目标，<br>softmax_cross_entropy_with_logits，多分类，结果只能属于一个类，</p>
<h3 id="训练步骤"><a href="#训练步骤" class="headerlink" title="训练步骤"></a>训练步骤</h3><p>总的步骤，样本预处理生成，设置输入和输出，设置参数，设置损失函数，分配喂入数据，</p>
<ul>
<li>样本预处理生成</li>
</ul>
<p>word2vec在skip-model中的样本是词对，center_word–&gt;target_word,</p>
<ul>
<li>1，读取训练集，是一个（或多个）文件的文本语料，将这些语料读取进一个list中；</li>
<li>2，把list中的词进行词频统计，然后递减排序，做成一个word<-->index_id对，即把词映射成id，</--></li>
<li>3，根据word<-->index_id对，得到词典，词与id的映射，id与词的映射关系，</--></li>
<li>4，依照skip数，生成样本，迭代的，做法有很多种，从list（步骤1得到的）依次取一个词，然后分别从该词的前后[1-skip]中随机取一个词作为目标词，这样就组成了一个样本center_word–&gt;target_word，而词也顺便转成id（步骤2和3得到的映射词典）。随机是避免陷入局部最优。</li>
<li>5，根据4，得到一系列的样本，即list[center_word–&gt;target_word],然后可以做批次取样本，喂入模型。</li>
</ul>
<ul>
<li>设置输入和输出</li>
</ul>
<p>模型的输入是中心词，输出是目标词，采用placeholders，</p>
<p>center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name=’center_words’)<br>target_words = tf.placeholder(tf.int32,shape=[BATCH_SIZE,1],name=’target_words’)</p>
<p>设置全量的词嵌入矩阵，<br>embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name=’embed_matrix’)</p>
<ul>
<li>设置参数</li>
</ul>
<p>采取一层网络，参数w和b，<br>nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE],stddev=1.0 / (EMBED_SIZE ** 0.5)), name=’nce_weight’)<br>nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name=’nce_bias’)</p>
<ul>
<li>损失函数</li>
</ul>
<p>先得到批次的向量，<br>embed = tf.nn.embedding_lookup(embed_matrix,center_words,name=’embed’)</p>
<p>tf.nn.embedding_lookup，根据input_ids中的id，寻找embedding中的对应向量（一行，从0开始计数）然后组成新的矩阵。</p>
<p>损失函数计算，<br>loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,<br>                                            biases=nce_bias,<br>                                            labels=target_words,<br>                                            inputs=embed,<br>                                            num_sampled=NUM_SAMPLED,#负样本数<br>                                            num_classes=VOCAB_SIZE), name=’loss’)</p>
<p>优化器，一般是批梯度下降法，<br>optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)</p>
<ul>
<li>喂入数据，迭代训练</li>
</ul>
<p>从样本预处理中，读取每个批次的样本，喂入模型中，</p>
<pre><code>with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    total_loss = 0.0 #总的loss
    writer = tf.summary.FileWriter(&apos;.my_graph/no_frills/&apos;, sess.graph)
    for index in range(NUM_TRAIN_STEPS):
        centers, targets = next(batch_gen) #已经生成的样本集合，不断循环取，
        loss_batch,_ = sess.run([loss,optimizer],feed_dict={center_words:centers,target_words:targets})

        total_loss += loss_batch
        if (index + 1) % SKIP_STEP == 0:
            print(&apos;Average loss at step {}: {:5.1f}&apos;.format(index, total_loss / SKIP_STEP))
            total_loss = 0.0
    writer.close()
</code></pre><ul>
<li>结果词向量</li>
</ul>
<p>最后训练完，把词向量保存，是embed_matrix;embed_matrix.eval()是numpy.ndarray类型。所以，不管训练用的多少层，只需记录最开始的词向量矩阵即可，</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>通过步骤的分解，整体的实现如下所示，含预处理和训练实现，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"></span><br><span class="line">from collections import Counter</span><br><span class="line">import random</span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">'..'</span>)</span><br><span class="line">import zipfile</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">from six.moves import urllib</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">import utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters for downloading data</span></span><br><span class="line">DOWNLOAD_URL = <span class="string">'http://mattmahoney.net/dc/'</span></span><br><span class="line">EXPECTED_BYTES = 31344016</span><br><span class="line">DATA_FOLDER = <span class="string">'data/'</span></span><br><span class="line">FILE_NAME = <span class="string">'text8'</span></span><br><span class="line"></span><br><span class="line">def download(file_name, expected_bytes):</span><br><span class="line">    <span class="string">""</span><span class="string">" Download the dataset text8 if it's not already downloaded "</span><span class="string">""</span></span><br><span class="line">    file_path = DATA_FOLDER + file_name</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(file_path):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Dataset ready"</span>)</span><br><span class="line">        <span class="built_in">return</span> file_path</span><br><span class="line">    <span class="built_in">print</span>(DOWNLOAD_URL + file_name)</span><br><span class="line">    file_name, _ = urllib.request.urlretrieve(DOWNLOAD_URL + file_name, file_path)</span><br><span class="line"></span><br><span class="line">    file_stat = os.stat(file_path)</span><br><span class="line">    <span class="keyword">if</span> file_stat.st_size == expected_bytes:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'Successfully downloaded the file'</span>, file_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        raise Exception(<span class="string">'File '</span> + file_name +</span><br><span class="line">                        <span class="string">' might be corrupted. You should try downloading it with a browser.'</span>)</span><br><span class="line">    <span class="built_in">return</span> file_path</span><br><span class="line"></span><br><span class="line">def read_data(file_path):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    大约有17,005,207词（含标点符号）</span></span><br><span class="line"><span class="string">    把所有的词读进一个list中，英文是空格分割，含标点符号，</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    with zipfile.ZipFile(file_path) as f:</span><br><span class="line">        words = tf.compat.as_str(f.read(f.namelist()[0])).split()</span><br><span class="line">        <span class="comment"># tf.compat.as_str() 输入的转换成string</span></span><br><span class="line">    <span class="built_in">return</span> words</span><br><span class="line"></span><br><span class="line">def build_vocab(words, vocab_size):</span><br><span class="line">    <span class="string">""</span><span class="string">" Build vocabulary of VOCAB_SIZE most frequent words</span></span><br><span class="line"><span class="string">    建立字典，给定的vocab_size大小，建立；按照频率选取前vocab_size个词，</span></span><br><span class="line"><span class="string">    返回词的索引，数组和字典，</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    dictionary = dict()</span><br><span class="line">    count = [(<span class="string">'UNK'</span>, -1)]</span><br><span class="line">    count.extend(Counter(words).most_common(vocab_size - 1))</span><br><span class="line">    index = 0</span><br><span class="line">    utils.make_dir(<span class="string">'processed'</span>)</span><br><span class="line">    with open(<span class="string">'processed/vocab_1000.tsv'</span>, <span class="string">"w"</span>) as f:</span><br><span class="line">        <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</span><br><span class="line">            dictionary[word] = index</span><br><span class="line">            <span class="keyword">if</span> index &lt; 1000:</span><br><span class="line">                f.write(word + <span class="string">"\n"</span>)</span><br><span class="line">            index += 1</span><br><span class="line">    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</span><br><span class="line">    <span class="built_in">return</span> dictionary, index_dictionary</span><br><span class="line"></span><br><span class="line">def convert_words_to_index(words, dictionary):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    把词替换成词典中的索引</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    <span class="built_in">return</span> [dictionary[word] <span class="keyword">if</span> word <span class="keyword">in</span> dictionary <span class="keyword">else</span> 0 <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line"></span><br><span class="line">def generate_sample(index_words, context_window_size):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    skip-gram模型，生成训练对，</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     "</span><span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> index, center <span class="keyword">in</span> enumerate(index_words):</span><br><span class="line">        context = random.randint(1, context_window_size)</span><br><span class="line">        <span class="comment"># 随机选择中心词的前一个后一个，还是前几个后几个，</span></span><br><span class="line">        <span class="keyword">for</span> target <span class="keyword">in</span> index_words[max(0, index - context): index]:</span><br><span class="line">            yield center, target</span><br><span class="line">        <span class="comment"># get a random target after the center wrod</span></span><br><span class="line">        <span class="keyword">for</span> target <span class="keyword">in</span> index_words[index + 1: index + context + 1]:</span><br><span class="line">            yield center, target</span><br><span class="line"></span><br><span class="line">def get_batch(iterator, batch_size):</span><br><span class="line">    <span class="string">""</span><span class="string">" Group a numerical stream into batches and yield them as Numpy arrays. "</span><span class="string">""</span></span><br><span class="line">    <span class="keyword">while</span> True:</span><br><span class="line">        center_batch = np.zeros(batch_size, dtype=np.int32)</span><br><span class="line">        target_batch = np.zeros([batch_size, 1])</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            center_batch[index], target_batch[index] = next(iterator)</span><br><span class="line">            <span class="comment">#next是从迭代器中挨个取值，这里是词对，生成的词对</span></span><br><span class="line">            <span class="comment">#中心词索引--&gt;前skip个（后skip个）词索引，这样的对，</span></span><br><span class="line">        yield center_batch, target_batch</span><br><span class="line"></span><br><span class="line">def process_data(vocab_size, batch_size, skip_window):</span><br><span class="line">    <span class="comment">#file_path = download(FILE_NAME, EXPECTED_BYTES)</span></span><br><span class="line">    file_path = <span class="string">"E:/workspace/DeepLearnings/tensorflow-learning/data/text8.zip"</span></span><br><span class="line">    <span class="built_in">print</span>(file_path)</span><br><span class="line">    words = read_data(file_path)<span class="comment">#读取训练所有词，存储在一个list中，</span></span><br><span class="line">    dictionary, _ = build_vocab(words, vocab_size)<span class="comment">#建立词典，根据指定的词典大小，词典的构建依赖于词在训练样本中的频率</span></span><br><span class="line">    index_words = convert_words_to_index(words, dictionary)<span class="comment">#得到词典，把训练集替换成索引，即文字--&gt;数字</span></span><br><span class="line">    del words <span class="comment"># 词存入内存</span></span><br><span class="line">    single_gen = generate_sample(index_words, skip_window)<span class="comment">#获取样本，根据中心词选取前后skip个词，构成词对，</span></span><br><span class="line">    <span class="built_in">return</span> get_batch(single_gen, batch_size)</span><br><span class="line"></span><br><span class="line">def get_index_vocab(vocab_size):</span><br><span class="line">    file_path = download(FILE_NAME, EXPECTED_BYTES)</span><br><span class="line">    words = read_data(file_path)</span><br><span class="line">    <span class="built_in">return</span> build_vocab(words, vocab_size)</span><br></pre></td></tr></table></figure></p>
<p>训练的实现，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="string">""</span><span class="string">" The mo frills implementation of word2vec skip-gram model using NCE loss.</span></span><br><span class="line"><span class="string">Author: Chip Huyen</span></span><br><span class="line"><span class="string">Prepared for the class CS 20SI: "</span>TensorFlow <span class="keyword">for</span> Deep Learning Research<span class="string">"</span></span><br><span class="line"><span class="string">cs20si.stanford.edu</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span class="string">'2'</span></span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.contrib.tensorboard.plugins import projector</span><br><span class="line"></span><br><span class="line">from process_data import process_data</span><br><span class="line"></span><br><span class="line">VOCAB_SIZE = 50000 <span class="comment">#词典大小</span></span><br><span class="line">BATCH_SIZE = 128 <span class="comment"># 每个批次的大小，即每个批次包含的样本量</span></span><br><span class="line">EMBED_SIZE = 128 <span class="comment"># 每个词的嵌入向量大小，即词向量维度</span></span><br><span class="line">SKIP_WINDOW = 1 <span class="comment"># 窗口，即中心词调几个词</span></span><br><span class="line">NUM_SAMPLED = 64    <span class="comment"># 抽样时取的负样本个数</span></span><br><span class="line">LEARNING_RATE = 1.0</span><br><span class="line">NUM_TRAIN_STEPS = 20000</span><br><span class="line">SKIP_STEP = 2000 <span class="comment"># 优化loss，训练多少次</span></span><br><span class="line"></span><br><span class="line">def word2vec(batch_gen):</span><br><span class="line">    <span class="string">""</span><span class="string">" Build the graph for word2vec model and train it "</span><span class="string">""</span></span><br><span class="line">    <span class="comment"># 使用placeholders设置输入和输出，即中心词和目标词</span></span><br><span class="line">    with tf.name_scope(<span class="string">'data'</span>):</span><br><span class="line">        center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name=<span class="string">'center_words'</span>)</span><br><span class="line">        target_words = tf.placeholder(tf.int32,shape=[BATCH_SIZE,1],name=<span class="string">'target_words'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词嵌入矩阵[词典大小,词向量大小]，</span></span><br><span class="line">    with tf.name_scope(<span class="string">'mbedding_matrix'</span>):</span><br><span class="line">        embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name=<span class="string">'embed_matrix'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型</span></span><br><span class="line">    <span class="comment"># tf.nn.embedding_lookup，根据input_ids中的id，寻找embedding中的对应向量（一行，从0开始计数）然后组成新的矩阵，</span></span><br><span class="line">    <span class="comment"># embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')</span></span><br><span class="line">    <span class="comment"># 开始训练，设置w和b，</span></span><br><span class="line">    with tf.name_scope(<span class="string">'loss'</span>):</span><br><span class="line">        embed = tf.nn.embedding_lookup(embed_matrix,center_words,name=<span class="string">'embed'</span>)</span><br><span class="line">        <span class="comment"># Step 4: construct variables for NCE loss</span></span><br><span class="line">        nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE],stddev=1.0 / (EMBED_SIZE ** 0.5)), name=<span class="string">'nce_weight'</span>)</span><br><span class="line">        nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name=<span class="string">'nce_bias'</span>)</span><br><span class="line">        <span class="comment"># nce损失函数</span></span><br><span class="line">        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,</span><br><span class="line">                                            biases=nce_bias,</span><br><span class="line">                                            labels=target_words,</span><br><span class="line">                                            inputs=embed,</span><br><span class="line">                                            num_sampled=NUM_SAMPLED,<span class="comment">#负样本数</span></span><br><span class="line">                                            num_classes=VOCAB_SIZE), name=<span class="string">'loss'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#迭代训练</span></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">        total_loss = 0.0 <span class="comment">#总的loss</span></span><br><span class="line">        writer = tf.summary.FileWriter(<span class="string">'.my_graph/no_frills/'</span>, sess.graph)</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(NUM_TRAIN_STEPS):</span><br><span class="line">            centers, targets = next(batch_gen)</span><br><span class="line">            loss_batch,_ = sess.run([loss,optimizer],feed_dict=&#123;center_words:centers,target_words:targets&#125;)</span><br><span class="line"></span><br><span class="line">            total_loss += loss_batch</span><br><span class="line">            <span class="keyword">if</span> (index + 1) % SKIP_STEP == 0:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">'Average loss at step &#123;&#125;: &#123;:5.1f&#125;'</span>.format(index, total_loss / SKIP_STEP))</span><br><span class="line">                total_loss = 0.0</span><br><span class="line">        writer.close()</span><br><span class="line">        <span class="comment">#最后训练完是embed_matrix;embed_matrix.eval()是numpy.ndarray类型</span></span><br><span class="line">def main():</span><br><span class="line">    batch_gen = process_data(VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW)</span><br><span class="line">    word2vec(batch_gen)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>把词转为词向量后，即把中文词转换成了数字，并且是一维向量，可以在很多数学模型中使用。</p>
<p>比如，作为其他模型的特征输入；向量之间计算词的相似性；通过词计算短语或句子的向量（当然有其他模型训练短语和句子的向量）；</p>
<p>总之，词向量的得到，可以作为其他应用的辅助或基础，另外，词向量的作用与lda-主题模型同理，可以得到词的向量表示。然而，主题模型，可以把词形象的标记在不同主题的概率，而词向量只是在同一空间内，把词进行重新分置，视具体的应用场景选择lda的向量表示还是词向量的表示。</p>
<h2 id="模型复用"><a href="#模型复用" class="headerlink" title="模型复用"></a>模型复用</h2><p>上述训练word2vec，可以看出代码没有类，使得代码在使用上欠缺复用性，下面把训练的模型做改变，</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class SkipGramModel:</span><br><span class="line">    def __init__(self, params):</span><br><span class="line">       pass</span><br><span class="line">    def _create_placeholders(self):</span><br><span class="line">       pass</span><br><span class="line">    def _create_embedding(self):</span><br><span class="line">       pass</span><br><span class="line">    def _create_loss(self):</span><br><span class="line">       pass</span><br><span class="line">    def _create_optimizer(self):</span><br><span class="line">       pass</span><br><span class="line">    def _create_summaries(self):</span><br><span class="line">       pass</span><br><span class="line">    def build_graph(self):</span><br><span class="line">       pass</span><br></pre></td></tr></table></figure>
<p>之后，把相关的逻辑填充在类的方法里即可。<a href="https://github.com/chlyzzo/DeepLearnings/blob/master/tensorflow-learning/word2vec/04_word2vec_visualize.py" target="_blank" rel="noopener">详细的代码实现</a>.</p>

	</div>
        
	<!-- css -->
	<style type="text/css">
	    .center {
	        text-align: center;
	    }
	    .hidden {
	        display: none;
	    }
		.donate_bar a.btn_donate{
			display: inline-block;
			width: 82px;
			height: 82px;
			background: url("http://7xsl28.com1.z0.glb.clouddn.com/btn_reward.gif") no-repeat;
			_background: url("http://7xsl28.com1.z0.glb.clouddn.com/btn_reward.gif") no-repeat;
		}

		.donate_bar a.btn_donate:hover{ background-position: 0px -82px;}
		.donate_bar .donate_txt {
			display: block;
			color: #9d9d9d;
			font: 14px/2 "Microsoft Yahei";
		}
		.bold{ font-weight: bold; }
	</style>
	<!-- /css -->

    <!-- Donate Module -->
    <div id="donate_module">

	<!-- btn_donate & tips -->
	<div id="donate_board" class="donate_bar center">
	    <br>
	    ------------------------------------------------------------------------------------------------------------------------------
	    <br>
		<a id="btn_donate" class="btn_donate" target="_self" href="javascript:;" title="打赏"></a>
		<span class="donate_txt">
			只卖艺不卖身，客官打赏，给您来段50的
		</span>
	</div>
	<!-- /btn_donate & tips -->

	<!-- donate guide -->

	<div id="donate_guide" class="donate_bar center hidden">
        <br>
	    ------------------------------------------------------------------------------------------------------------------------------
	    <br>
	    
	    <div width="100%" align="center"><div name="dashmain" id="dash-main-id-87895f" class="dash-main-3 87895f-0.99"></div></div>
		<script type="text/javascript" charset="utf-8" src="http://www.dashangcloud.com/static/ds.js"></script>
		

		<a href="http://chloy.com/images/wechatpay.png" title="微信扫一扫" class="fancybox" rel="article0">
			<img src="http://chloy.com/images/wechatpay.png" title="微信打赏" height="190px" width="auto"/>
		</a>

        &nbsp;&nbsp;

		<a href="http://chloy.com/images/alipay.png" title="支付宝扫一扫" class="fancybox" rel="article0">
			<img src="http://chloy.com/images/alipay.png" title="支付宝打赏" height="190px" width="auto"/>
		</a>

		<span class="donate_txt">
			只卖艺不卖身，客官打赏，给您来段50的
		</span>

	</div>
	<!-- /donate guide -->

	<!-- donate script -->
	<script type="text/javascript">
		document.getElementById('btn_donate').onclick = function() {
			$('#donate_board').addClass('hidden');
	    $('#donate_guide').removeClass('hidden');
		}

		function donate_on_web(){
			$('#donate').submit();
        }

		var original_window_onload = window.onload;
        window.onload = function () {
            if (original_window_onload) {
                original_window_onload();
            }
            document.getElementById('donate_board_wdg').className = 'hidden';
		}
	</script>
	<!-- /donate script -->
</div>
<!-- /Donate Module -->

	<footer class="article-footer clearfix">

	<div class="article-share" id="share">
	
	  <div data-url="https://chlyzzo.github.io/2017/09/28/tensorflow学习-word2vec/" data-title="tensorflow学习-word2vec | 山上掏金" data-tsina="1724571293" class="share clearfix">
	  </div>
	
	</div>


</footer>


	</article>
        




    <div id="gitalk-container" style="display: inline-block; width: 90%; margin-left: 4%;"></div>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <script src="/js/gitalk.js"></script>
    <script src="/js/md5.min.js"></script>
    <script type="text/javascript">
        var gitalk = new Gitalk({
            clientID: '9cc4ad70a7ea08a5e2ee',
            clientSecret: '90a96ec418401a19b45f8d76efaf3a345aec3367',
            repo: 'chlyzzo.github.io',
            id: md5(location.pathname),
            distractionFreeMode: true,
            owner: 'chlyzzo',
            admin: ['chlyzzo']
        });
        gitalk.render('gitalk-container')
    </script>

	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2017/10/10/tensorflow-卷积/" title="tensorflow-卷积">
  <strong>上一篇：</strong><br/>
  <span>
  tensorflow-卷积</span>
</a>
</div>


<div class="next">
<a href="/2017/09/21/通过python，java，spark，hive以post提交数据/"  title="通过python，java，spark，hive以post提交数据">
 <strong>下一篇：</strong><br/> 
 <span>通过python，java，spark，hive以post提交数据
</span>
</a>
</div>

</nav>

</div>

    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> 分享即是收获，动手后才是自己的. <br/>
			</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		<a href="http://weibo.com/1724571293" target="_blank" class="icon-weibo" title="微博"></a>
		
		
		<a href="https://github.com/chlyzzo" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:rimin515@sina.cn" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		<a href="http://www.miitbeian.gov.cn/publish/query/indexFirst.action">沪ICP备18018970号</a>
		
		<a href="/about" target="_blank" title="chloy">chloy</a>
		
	        <a href="https://pages.coding.me">Hosted by Coding Pages</a>
		</p>

</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  c.click(function(){
    ta.css('display', 'block').addClass('fadeIn');
  });
  o.click(function(){
    ta.css('display', 'none');
  });
  $(window).scroll(function(){
    ta.css("top",Math.max(140,320-$(this).scrollTop()));
  });
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina'),
      description = $this.attr('description');
  var html = [
  '<div class="hoverqrcode clearfix"></div>',
  '<a class="overlay" id="qrcode"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="微信"></a>',
  '<a href="http://widget.renren.com/dialog/share?resourceUrl=' + encodedUrl + '&srcUrl=' + encodedUrl + '&title=' + title +'" class="article-share-renren" target="_blank" title="人人"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="微博"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);

  $('.hoverqrcode').hide();

  var myWidth = 0;
  function updatehoverqrcode(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
    var qrsize = myWidth > 1024 ? 200:100;
    var options = {render: 'image', size: qrsize, fill: '#2ca6cb', text: url, radius: 0.5, quiet: 1};
    var p = $('.article-share-qrcode').position();
    $('.hoverqrcode').empty().css('width', qrsize).css('height', qrsize)
                          .css('left', p.left-qrsize/2+20).css('top', p.top-qrsize-10)
                          .qrcode(options);
  };
  $(window).resize(function(){
    $('.hoverqrcode').hide();
  });
  $('.article-share-qrcode').click(function(){
    updatehoverqrcode();
    $('.hoverqrcode').toggle();
  });
  $('.article-share-qrcode').hover(function(){}, function(){
      $('.hoverqrcode').hide();
  });
});   
</script>











<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?81e8b61e30a0723ad6270902dbcf0bb6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>




<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->
  </body>
</html>
